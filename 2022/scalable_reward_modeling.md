# Scalable reward modeling

https://arxiv.org/pdf/1811.07871v1.pdf


In practice, we train more capable and general agents a_{k+1} from previous agent a_k. We produce C(a_k) to critique a_{k+1}'s actions/ output, enforcing it to adhere to our preferences, which 

Confusion: A_4, A_3 and A_2. We train A_3 critiqued with A_2. Now A_3 knows. When we then train A_4: How does it know to 'retain' the properties that A_2 taught A_3 ? Should we not create several critics C(A_2), C(A_3) and use them in conjunction to critique A_4 ?

Interesting analogy to P vs NP: Evaluating behavior is polynomial, generating that behavior has exponential cost

Got to about page 9 atm

TO BE READ: (https://arxiv.org/pdf/1810.08575.pdf AND https://www.google.com/search?q=AI+safety+via+debate.&sourceid=chrome&ie=UTF-8)